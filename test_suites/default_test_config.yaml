# YAML        

#   Default test configuration file, vers. 0.4 

# --------  Basic Instructions -------------

# This file contains what a single entry in a user test suite 
# CAN look like. Each entry in the user test suite will inherit
# its parts from this file. Thus, if you want to effect all the
# entries in the user test suite, change this file, otherwise change
# only the appropriate parts of the user test suite.

# NOTE
#  - All variables in CAPS are nix environment vars
#  - Lines with a "#" are comments.

# A new named stanza or section is needed for each new test,
# but you only need to define what's different from the
# this default test_config file. Simply ommit the part(s) you want to
# inherit.
# In theory, ever part of the default file can be re-used, but it's necessary to 
# at least changing the Id, name, location, and run:cmd sub-elements for each
# new test.
#

# for YAML formatting issues try: http://yaml-online-parser.appspot.com
    

# ---------- Section begin -------------------------------

# Default testSuite override.
# The software will look for the default file in the same directory as the
# user defined test suite. However, to use a different one 
# add/modify this entry in the user test suite file.
# Use a fully qualified path/name and file.
# DefaultTestSuite: /mydir/test_suites/default_test_suite.yaml

# Unique (to the test suite being used) test/job identifier. Any string will do.
# DO NOT add a value to right side of the ":"
UniqueId:

  # whatever you want to call your test/job.
  # ENV variable PV_TESTNAME will contain this string at run time.
  name: TestName

  # uri of test, probably a directory name  
  source_location: HOME
        
  # Build Section.  
  build:
    # The software will call this cmd before each run.
    # command is relative to source directory
    cmd: 'buildme'
    build_before_run_flag : False
        
  # Run Section 
  run:
    # an executable file name
    cmd: 'runme'
    # select one scheduler type  
    # choices currently are moab and raw  
    # Consider adding a new type if you want to contribute to this
    # framework.
    scheduler: 'moab'
    # user specified test specific argument string
    # ENV variable PV_TEST_ARGS will contain this string at run time.
    test_args: '' 
    # number of times to repeat this section
    count: 1

  # Optional scheduler section needed if defined in the "run" section
  moab:
    # Parameter(s) provided to msub command 
    # This section is used if this scheduler chosen for this test.
    # All combinations of <num_nodes> and <procs_per_node> will be
    # tried by by the scheduler where it makes sense. A "none"
    # choice simply invokes the run command once

    # comma separated list of values, or range 
    num_nodes: 1
    # queue to submit jobs against (optional)
    queue: "" 
    # reservation to submit jobs against (optional)
    reservation: "" 
    # comma separated list of values, or range 
    procs_per_node: 16
    # time in hr::min::secs
    time_limit: 01:00:00
    # free formatted string added to msub invocation line (optional)
    msub_args:
    # target segment, will be added to feature argument (optional)
    target_seg: "" 
    # optional list of nodes to target (not yet implemented) 
    node_list: "" 
    # Percent of cluster consumed by this particular test in this
    # test suite of jobs. Noralization will occurr. (optional)
    percent: ""
    

  # Working space section
  # By default each job will be copied to and run from a temporary "working space".
  # This keeps subsequent jobs from colliding with one another.
  # If path is left null ("") then this is not done, otherwise
  # it will be set relative to the source directory or can be 
  # a fully specified directory.

  working_space:
    # path to working space.
    # ENV variable PV_WS will contain the full working space path at run time.
    path: 'pv_ws' 
    #path: '/pv_ws' 
    # comma separated list of files to copy to the working space before
    # the test is launched.
    #  *.[ocfh], *.bck, and *.tar files are never copied.
    copy_to_ws: ''
    # comma separated list of files to copy to the results directory after
    # the job completes. *.log and *.std* files will always copied.
    save_from_ws: ''
    no_copy: '*.c,*.o,*.h'


  # Results section
  results:

    # The test handler places all results (for this test) under a dynamically
    # created directory. The name for this directory is stored in the
    # ENV variable PV_JOB_RESULTS_LOG_DIR, under the root directory stored in
    # the ENV variable PV_RESULT_ROOT 
    root: '/Users/cwi/pv_results'

    # Pass or Fail matching pattern.
    # The test handler will try to match "<result> pass" or
    # <result> fail" to determine if the test passed or failed.
    pass_fail_regex: '<result\w{0,1}>\s*(.+)'

    # Trend Data matching pattern.
    # The test handler will try to match "<td> name value [units]"
    # to discover any special result data  (a.k.a - trend data).
    # The "units" component is optional.
    trend_data_regex: <td>\s+(.*)

    # A Script called at the end of the test/job.
    # By default, the test harness requires an entry in the job output
    # (see pass or faile matching pattern) located at the beginning of a line
    # followed by a pass or fail. Of course, there are many ways to provide this,
    # but if it's not added by the job itself this may be a convient way to post-process 
    # the results and add this entry.
    # *** If defined, make sure it's executable  
    epilog_script: "" 

    # Script that contains the logic that examines the job log 
    # to see if the test passed or failed.  chklg reports
    # a fail if there are any "fail" matches with the
    # pass_fail_regex, OR it can report pass if at least one "pass"
    # is matched, otherwise unknown is reported.
    # DO NOT change at this time!
    pass_fail_logic: chklg

  # LDMS tool settings.
  # This tool can generate a great deal of data!
  ldms:
    # used to set env var LDMS_HOME
    install_dir : "/usr/projects/hpctools/ldms"
    # command to start up processes on nodes
    start_cmd: startJobNodesLDMSD
    # root directory where all output will go 
    output_dir_root: HOME 
    # sample rate
    freq: normal
    # comman separated list of plugin sampler names (all coming soon),
    # will be skipped if not supported
    metric_list: "meminfo,vmstat"
    
  time:
    tz: 'US/Mountain'
