# YAML        
# ------------ Test Stanza Definition ---------------------

# Unique (to the test suite being used) test/job identifier. Any string will do.
# DO NOT add a value to right side of the ":"
UniqueId:

  # whatever you want to call your test/job.
  # ENV variable PV_TESTNAME will contain this string at run time.
  # DO NOT USE any double under scores "__" in this name!
  name: TestName

  # uri of test, probably a directory name  
  source_location: HOME
  source:
    root: ''
    rpath: ''
        
  # Build Section.  
  build:
    # The software will call this cmd before each run.
    # command is relative to source directory
    cmd: 'buildme'
    build_before_run_flag : False
        
  # Run Section 
  run:
    # an executable file name
    # DO NOT USE any double under scores "__" in this name!
    cmd: 'runme'
    # select one scheduler type  
    # choices currently are moab and raw  
    # Consider adding a new type if you want to contribute to this
    # framework.
    scheduler: 'slurm'
    # User specified test specific argument string
    # The ENV variable PV_TEST_ARGS will contain this string at run time.
    test_args: '' 
    # number of times to repeat this section
    count: 1


  # Optional scheduler section needed if defined in the "run" section
  moab:
    # Parameter(s) provided to msub command 
    # This section is used if this scheduler chosen for this test.
    # All combinations of <num_nodes> and <procs_per_node> will be
    # tried by by the scheduler where it makes sense. A "none"
    # choice simply invokes the run command once
    # Number of nodes to allocate for this test.
    # This entry must be an integer or it can be a
    # list "[]" of comma separated integers to try variations.
    num_nodes:  ""
    # queue to submit jobs against (optional)
    queue: ""
    # reservation to submit jobs against (optional)
    reservation: "" 
    # Number of processors defined per node for this test 
    # This entry must be an integer or it can be a 
    # list "[]" of comma separated integers to try variations.
    procs_per_node: 16
    # time in hr:min:secs
    # This SHOULD be quoted
    time_limit: "01:00:00"
    # free formatted string added to msub invocation line (optional)
    msub_args:
    # target segment, added as a feature component to the "-l" argument (optional)
    target_seg: "" 
    # optional list of nodes to target 
    # Uses defined string to populate '-l nodes=<node_list>' on msub line
    # Overrides number of nodes requested.
    node_list: "" 
    # Percent of cluster consumed by this particular test in this
    # test suite of jobs. Noralization will occurr. (optional)
    percent: ""

  slurm:
    # Number of nodes to request from the queueing system.  Allows for an array of
    # integers or simply 'all'.  Values should be seperated by a comma.
    num_nodes: 1600
    # Number of processors per node to request from the queueing system.  Accepts an
    # integer.
    procs_per_node: 272
    # Maximum time to request from the queueing system.  If the job is not completed
    # by the end of this time period, the job will be killed by the scheduler.
    time_limit: "332:00:00"
    # Specifies the partition to submit the job to.  In the case of Trinitite, this
    # could be 'knl' or 'haswell'.  In most cases, the main partition is 'standard'
    # or 'any'.
    partition: any
    # Specifies the account that the request is sent from.  This will determine
    # the list of possibilities for the 'qos' option.  Typically, the options used
    # here are 'hpctest' and 'hpcdev'.
    account: hpctest
    # Specifies the QOS to use.  The available QOSes can be viewed using the command
    # `sacctmgr show assoc user=$USER -p`.  One of the first columns will be the
    # account specified by the previous entry.  One of the final columns will be a
    # comma-delimited list of possible QOS values for that account.  Examples include
    # 'high', 'tossdev' (toss systems only), 'long', 'large', 'standard', and 'yield'.
    # The QOS will determine how quickly your job will get through the queue and how
    # nice it will be to other jobs.
    qos: standard

  raw:
    # Mostly a placeholder
    num_nodes: 1
    

  # Working space section
  # By default the test source tree is copied to and run from a temporary "working space".
  # This keeps simultaneous running jobs from colliding with one another.
  # This step can be ignored if the nocopy flag is set to True. 

  working_space:
    # Path to copy to...
    # At run time the src test directory files(s) are copied to this directory.
    # By default this is placed under the src test directory, but if qualified with a full
    # path will be placed there.
    # The ENV variable PV_WS will contain the full path of this working space at run time.
    path: '/usr/projects/hpctools/PAV_PRETEAM/workingspace/pv_ws' 
    nocopy: False
    # Override the added path component to the working space root. Default is "pv_ws".
    rpath: 'pv_ws'
    # Override files to copy to the Working Space
    # By default, ALL files are copied except for *.[ocfh], *.bck, and *.tar files.
    # Comma separated list of specific files or dirs to copy from the test/src directory
    # to the working space before the test is launched.
    # Basically it's the command "rsync -a src dst".
    copy_to_ws: ''
    # Save from the Working Space
    # Comma separated list of files to copy from the working space to the
    # final results directory after the job completes.
    # This list is for the user to define extra files that are unique to this job that
    # they want to save. Default is to copy NO files.
    
    # Basically its the Unix command "cp src dst". 
    save_from_ws: '*'


  # Results section
  results:

    # The test handler places all results under a root result
    # directory, defined here. Its value is stored in
    # the ENV variable PV_RESULT_ROOT at run time. When "pav get_results" is used 
    # any result data found (i.e. from other test runs) in this space will be examined too.
    # DO NOT use ENV parameters in this path ($HOME for instance)
 #   root: '/Users/cwi/pv_results'
 #   per README.txt, changing the above
    root: '/usr/projects/hpctools/PAV_PRETEAM/results'

    # Pass or Fail matching pattern, not case sensitive.
    # The test handler will try to match "<result> pass" or
    # <result> fail" in the jobs output to determine if the
    # test passed or failed.
    # If multiple "results" are present any fail
    # will constitute a failure. See pass_fail_logic description.
    pass_fail_regex: '<result\w{0,1}>\s*(.+)'

    # Trend Data matching pattern.
    # The test handler will try to match "<td> name value [units]"
    # to discover any special result data  (a.k.a - trend data).
    # The "units" component is optional.
    # DO NOT change at this time!
    trend_data_regex: '^<td>\s+(.*)'

    # Optional script called at the end of the test/job.
    # By default, the test harness requires a line in the job standard output
    # to signifiy pass or fail
    # Of course, there are many ways to provide this, but if it's not added by the core
    # program itself this may be a convient way to post-process 
    # the results and add this entry.
    # *** If defined, make sure it's executable!
    epilog_script: "" 

    # Script that contains the logic that examines the job log 
    # to see if the test passed or failed.  chklg reports
    # a fail if there are any "fail" matches against the
    # pass_fail_regex, otherwise it reports a pass if at least one "pass"
    # is matched, otherwise unknown is reported.
    # DO NOT change at this time!
    pass_fail_logic: chklg

  # LDMS tool settings.
  # This tool can generate a great deal of data!
  ldms:
    # used to set env var LDMS_HOME
    install_dir: "/usr/projects/hpctools/ldms"
    # Turn on or off ( "-m" flag wil turn on for all apps/test
    # in the test suite.) 
    state: off
    # command to start up processes on nodes
    start_cmd: startJobNodesLDMSD
    # root directory where all output will go 
    output_dir_root: HOME 
    # sample rate
    freq: normal
    # comman separated list of plugin sampler names (all coming soon),
    # will be skipped if not supported
    metric_list: "meminfo,vmstat"

  # Splunk configuration setup
  splunk:
    # Turn on or off
    # If on, creates a Splunk data file in the job/app result directory 
    # and concatenates, or creates if necessary, this data to the
    # global splunk data file.
    #state: off 
    state: on 
    # Define the location of the global splunk data file
    #global_data_file: '/Users/cwi/pv_results/splunkdata.log'
 #   per README.txt, changing the above to....
    global_data_file: '/usr/projects/splunk/results/hpctest/moonlight/splunkdata.log'
    
  time:
    tz: 'US/Mountain'
