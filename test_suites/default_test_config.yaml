# YAML        

#  ###################################################################
#
#  Disclaimer and Notice of Copyright 
#  ==================================
#
#  Copyright (c) 2015, Los Alamos National Security, LLC
#  All rights reserved.
#
#  Copyright 2015. Los Alamos National Security, LLC. 
#  This software was produced under U.S. Government contract 
#  DE-AC52-06NA25396 for Los Alamos National Laboratory (LANL), 
#  which is operated by Los Alamos National Security, LLC for 
#  the U.S. Department of Energy. The U.S. Government has rights 
#  to use, reproduce, and distribute this software.  NEITHER 
#  THE GOVERNMENT NOR LOS ALAMOS NATIONAL SECURITY, LLC MAKES 
#  ANY WARRANTY, EXPRESS OR IMPLIED, OR ASSUMES ANY LIABILITY 
#  FOR THE USE OF THIS SOFTWARE.  If software is modified to 
#  produce derivative works, such modified software should be 
#  clearly marked, so as not to confuse it with the version 
#  available from LANL.
#
#  Additionally, redistribution and use in source and binary 
#  forms, with or without modification, are permitted provided 
#  that the following conditions are met:
#
#  1. Redistributions of source code must retain the 
#     above copyright notice, this list of conditions 
#     and the following disclaimer. 
#  2. Redistributions in binary form must reproduce the 
#     above copyright notice, this list of conditions 
#     and the following disclaimer in the documentation 
#     and/or other materials provided with the distribution. 
#  3. Neither the name of Los Alamos National Security, LLC, 
#     Los Alamos National Laboratory, LANL, the U.S. Government, 
#     nor the names of its contributors may be used to endorse 
#     or promote products derived from this software without 
#     specific prior written permission.
#   
#  THIS SOFTWARE IS PROVIDED BY LOS ALAMOS NATIONAL SECURITY, LLC 
#  AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, 
#  INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF 
#  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. 
#  IN NO EVENT SHALL LOS ALAMOS NATIONAL SECURITY, LLC OR CONTRIBUTORS 
#  BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, 
#  OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, 
#  OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY 
#  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR 
#  TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT 
#  OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY 
#  OF SUCH DAMAGE.
#
#  ###################################################################


#   Default test configuration file, vers. 0.7 

# --------  Basic Instructions -------------

# This file contains what a complete single stanza in a user test suite 
# will contain. Each stanza (test entry), of which there can be as many as
# wanted in the user test suite, will inherit each of its components
# from this configuration, unless re-defined.
# Therefore, if you want to effect all the entries during all runs,
# change this file, otherwise change only the appropriate parts of the user test suite.

# NOTE
#  - All variables in CAPS are nix environment vars
#  - Lines with a "#" are comments.

# For each new test stanza, only define what's different from the
# this default test_config file. Simply ommit the part(s) you want to
# inherit.
# In theory, ever part of the default file could be re-used, in practice it's
# necessary to at least changing the Id, name, location, and run:cmd sub-elements
# for each new test.
#

# for YAML formatting issues try: http://yaml-online-parser.appspot.com

# It is HIGHLY recommended that you make a copy of this file and tweak it as
# needed and make this new file the default_test_config.yaml file.
    

# ---------- Special Directives Section -------------------------------

# These are examples of reserved directives that can be added to the user test suite.
# DO NOT use them in the default config file(s).

# Override the default testSuite.
#
# Pavilion looks for the default test file definition in the same directory as the
# user test suite, given as the input argument.
# To use a different default file, place this directive 
# in the user test suite file. Use a fully qualified path/name and file.
# DefaultTestSuite: new_default_test_suite.yaml
#
# Include other test suites to create a "Super" test suite
# 
# To build up a test suite from multiple files, include a comma separated list of 
# file names including the relative path to the main user file.
# Two levels deep are supported.
# IncludeTestSuite: [ first_test_suite.yaml, second_test_suite, ...] 



# ------------ Test Stanza Definition ---------------------

# Unique (to the test suite being used) test/job identifier. Any string will do.
# DO NOT add a value to right side of the ":"
UniqueId:

  # whatever you want to call your test/job.
  # ENV variable PV_TESTNAME will contain this string at run time.
  # DO NOT USE any double under scores "__" in this name!
  name: TestName

  # uri of test, probably a directory name  
  source_location: HOME
  source:
    root: ''
    rpath: ''
        
  # Build Section.  
  build:
    # The software will call this cmd before each run.
    # command is relative to source directory
    cmd: 'buildme'
    build_before_run_flag : False
        
  # Run Section 
  run:
    # an executable file name
    # DO NOT USE any double under scores "__" in this name!
    cmd: 'runme'
    # select one scheduler type  
    # choices currently are moab and raw  
    # Consider adding a new type if you want to contribute to this
    # framework.
    scheduler: 'moab'
    # User specified test specific argument string
    # The ENV variable PV_TEST_ARGS will contain this string at run time.
    # This can be two dimensional where all the combinations of the lists will
    # be used. For example [['a', 'b'], ['c', 'd']] will exand to:
    # ['a c', 'a d', 'b c', 'b d']
    test_args: ['']
    # restrictions on the test arguments this is a list of sets that you want to
    # exclude. For example: ['a c', 'b d'] will cause the list from the example
    # above to be ['a d', 'b c']
    # an empty string will block all arguments
    test_args_restrictions: []
    # number of times to repeat this section
    count: 1

  # Optional scheduler section needed if defined in the "run" section
  moab:
    # Parameter(s) provided to msub command 
    # This section is used if this scheduler chosen for this test.
    # All combinations of <num_nodes> and <procs_per_node> will be
    # tried by by the scheduler where it makes sense. A "none"
    # choice simply invokes the run command once

    # Number of nodes to allocate for this test.
    # This entry must be an integer or it can be a
    # list "[]" of comma separated integers to try variations.
    num_nodes:  [ 1, 2 ]
    # queue to submit jobs against (optional)
    queue: "" 
    # reservation to submit jobs against (optional)
    reservation: "" 
    # Number of processors defined per node for this test 
    # This entry must be an integer or it can be a 
    # list "[]" of comma separated integers to try variations.
    procs_per_node: 16
    # time in hr:min:secs
    # This SHOULD be quoted
    time_limit: "01:00:00"
    # free formatted string added to msub invocation line (optional)
    msub_args:
    # target segment, added as a feature component to the "-l" argument (optional)
    target_seg: "" 
    # optional list of nodes to target 
    # Uses defined string to populate '-l nodes=<node_list>' on msub line
    # Overrides number of nodes requested.
    node_list: "" 
    # Percent of cluster consumed by this particular test in this
    # test suite of jobs. Noralization will occurr. (optional)
    percent: ""
    # the type of machine to run on (eg: KNL)
    machine_type: ""
    # os allows specification of the os (eg: -l nodes=4:KNL,os=OSSTUFF)
    os: ""


  # Optional scheduler section needed if defined in the "run" section
  slurm:
    # Parameter(s) provided to sbatch command 
    # This section is used if this scheduler chosen for this test.
    # All combinations of <num_nodes> and <procs_per_node> will be
    # tried by by the scheduler where it makes sense. A "none"
    # choice simply invokes the run command once

    # Number of nodes to allocate for this test.
    # This entry must be an integer or it can be a
    # list "[]" of comma separated integers to try variations.
    num_nodes:  [ 1, 2 ]
    # reservation to submit jobs against (optional)
    reservation: "" 
    # Number of processors defined per node for this test 
    # This entry must be an integer or it can be a 
    # list "[]" of comma separated integers to try variations.
    procs_per_node: 16
    # time in hr:min:secs
    # This SHOULD be quoted
    time_limit: "01:00:00"
    # target segment, added as a feature component to the "-l" argument (optional)
    target_seg: "" 
    # optional list of nodes to target 
    # Uses defined string to populate '-w <node_list>' on sbatch line
    # This can also be a list
    # For example nodelist: ['ca002','ca003','ca005']
    # This will launch 3 jobs. One on ca002, one on ca003, and one on ca005
    # Overrides number of nodes requested.
    node_list: ['']

  raw:
    # Mostly a placeholder
    num_nodes: 1
    

  # Working space section
  # By default the test source tree is copied to and run from a temporary "working space".
  # This keeps simultaneous running jobs from colliding with one another.
  # This step can be ignored if the nocopy flag is set to True. 

  working_space:
    # At run time the src test directory is copied to this directory.
    # By default this is placed under the src test directory, but if qualified with a full
    # path it will be placed there. If left null (''), the src dir is the working space and no copy 
    # is performed.
    # The ENV variable PV_WS will be set to the full path of this working space dir at run time.
    # By default, ALL files are copied except for *.[ocfh], *.bck, and *.tar files.
    path: 'pv_ws' 
    # Override files to copy to the Working Space with
    # copy_to_ws, if set, specifies what files or dirs to move to the working space.
    # Basically it's the command "rsync -a src dst".
    copy_to_ws: ''
    # Save from the Working Space
    # Comma separated list of files to copy from the working space to the
    # final results directory after the job completes.
    # This list is for the user to define extra files that are unique to this job that
    # they want to save. Default is to copy NO files.
    # Basically it's the Unix command "cp src dst". 
    save_from_ws: ''


  # Results section
  results:

    # The test handler places all results under a root result
    # directory, defined here. Its value is stored in
    # the ENV variable PV_RESULT_ROOT at run time. When "pav get_results" is used 
    # any result data found (i.e. from other test runs) in this space will be examined too.
    # DO NOT use ENV parameters in this path ($HOME for instance)
    root: '/Users/cwi/pv_results'

    # Pass or Fail matching pattern, not case sensitive.
    # The test handler will try to match "<result> pass" or
    # <result> fail" in the jobs output to determine if the
    # test passed or failed.
    # If multiple "results" are present any fail
    # will constitute a failure. See pass_fail_logic description.
    pass_fail_regex: '<result\w{0,1}>\s*(.+)'

    # Trend Data matching pattern.
    # The test handler will try to match "<td> name value [units]"
    # to discover any special result data  (a.k.a - trend data).
    # The "units" component is optional.
    # DO NOT change at this time!
    trend_data_regex: '^<td>\s+(.*)'

    # Optional script called at the end of the test/job.
    # By default, the test harness requires a line in the job standard output
    # to signifiy pass or fail
    # Of course, there are many ways to provide this, but if it's not added by the core
    # program itself this may be a convient way to post-process 
    # the results and add this entry.
    # *** If defined, make sure it's executable!
    epilog_script: "" 

    # Script that contains the logic that examines the job log 
    # to see if the test passed or failed.  chklg reports
    # a fail if there are any "fail" matches against the
    # pass_fail_regex, otherwise it reports a pass if at least one "pass"
    # is matched, otherwise unknown is reported.
    # DO NOT change at this time!
    pass_fail_logic: chklg

  # LDMS tool settings.
  # This tool can generate a great deal of data!
  ldms:
    # used to set env var LDMS_HOME
    install_dir: "/usr/projects/hpctools/ldms"
    # Turn on or off ( "-m" flag wil turn on for all apps/test
    # in the test suite.) 
    state: off
    # command to start up processes on nodes
    start_cmd: startJobNodesLDMSD
    # root directory where all output will go 
    output_dir_root: HOME 
    # sample rate
    freq: normal
    # comman separated list of plugin sampler names (all coming soon),
    # will be skipped if not supported
    metric_list: "meminfo,vmstat"

  # Splunk configuration setup
  splunk:
    # Turn on or off
    # If on, creates a Splunk data file in the job/app result directory 
    # and concatenates, or creates if necessary, this data to the
    # global splunk data file.
    state: off 
    # Define the location of the global splunk data file
    global_data_file: '/Users/cwi/pv_results/splunkdata.log'
    
  time:
    tz: 'US/Mountain'
