Pavilion (Gazebo-II) Test Handler Functional Requirements/Goals
Ver 1.6

 - This is the basic wish list (not in any particular priority) for what is desired
   in Pavilion.



1. Functionality that exists in current version of Gazebo and is to be continued.
-----------


1.1 Run as a user process.  Gracefully handles permissions to support multiple users running out of the same installation space.   
1.2 Placeholders for results created immediately upon launch
1.3 Supports a continual testing mode
1.4 Supports testing at a targeted load level
1.5 Supports a delay time between test suite launches
1.6 Defined convention to automate data collection of test output results (for example, current trend data tags)
1.7 Maintains enough data and provides tools to generate system coverage statistics
1.8 Maintains enough data and provides tools to generate individual test run-time stats.
1.9 Maintains enough data and provides tools for summarizing passing and failing results with runtime statistics
1.10 Supports efficient time based analysis (user selectable results to any time frame)  
1.11 Hierarchical directory store for raw data to facilite easy accss to results



2. Feature exists in some form in Gazebo, but needs improvements/enhancements
---------

2.1 Support for Moab and other schedulers/resource manager combinations.
2.2 Support for running simultaneous single or multiple test suites.  
2.3 Support for targeting nodes and segments, where possible.  
2.4 Minimize job launch script footprint called from DRM
2.5 Support run-time test instance copy to new run location (is default now) 
2.6 Supports build and then run mode, as well as running existing binaries
2.7 Support pre-run of resource availability before launch ( i.e. - go/no-go decision to run based on available file space )
2.8 Better user documentation / developer documentation
2.9 Provide some generic test code(s) (gcc based?) as example(s).
2.10 Tools to automate generating baseline performance results. 
2.11 Performance learning capability to determine threshold for pass/fail. (Polish up work done by Evan) 
2.12 Summary result reporting
2.13 Automated fail analysis reporting based on known failures (knowledge based analysis)
2.14 Automated performance data summaries and charting
2.15 Simpler design for making it easy to add tests



3. New Requirements
------------
 
3.1 Support multiple test managers running simultaneously (continuous launching tool)
3.2 Support multiple master test suite configuration file for feeding a variety of parameters to tests.  
3.3 Support testing front-ends as well as compute nodes thus facilitating testing fta's too
3.4 Elegant way of handling combinatorial parameter studies. 
3.5 Mechanism to hook in measurement/profiling tools ( power aware, LDMS, sensors, profiling, etc.)   
3.6 Support multiple compilers libraries and optimization levels and flags for one test instance, not requiring new test directories (same as # 10 ??)
3.7 GUI job monitoring / launching / results analysis mechanism
3.8 Singular command line interface providing comprehensive list of features
3.9 System component discovery mechanism (both hardware and software) with autopopulation of main config file where appropriate. (CBTF?)
3.10 Modular design/architecture for ease of co-development the harness.
3.11 Functions have self test or unit test built-in for regression test verification. 
3.12 Portable across Linux based clusters  (not dependent on specific site installations )
3.13 Maintain as much compatibiliy with Gazebo so that old tools and new tools can be used to analyze results generated by either. 
